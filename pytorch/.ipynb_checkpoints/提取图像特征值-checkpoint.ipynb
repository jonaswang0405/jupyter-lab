{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef74464c-0245-433b-9a99-ca81303c4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
    "from torchvision.models.detection.backbone_utils import LastLevelMaxPool\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "m=resnet50()\n",
    "train_nodes,eval_nodes=get_graph_node_names(m)\n",
    "\n",
    "\n",
    "\n",
    "return_nodes={\n",
    "    'layer1.2.relu_2':'layer1',\n",
    "    'layer2.3.relu_2':'layer2',\n",
    "    'layer3.5.relu_2':'layer3',\n",
    "    'layer4.2.relu_2':'layer4',\n",
    "}\n",
    "\n",
    "return_nodes = {\n",
    "    'layer1': 'layer1',\n",
    "    'layer2': 'layer2',\n",
    "    'layer3': 'layer3',\n",
    "    'layer4': 'layer4',\n",
    "}\n",
    "\n",
    "create_feature_extractor(m, return_nodes=return_nodes)\n",
    "\n",
    "class Resnet50WithFPN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet50WithFPN, self).__init__()\n",
    "        # Get a resnet50 backbone\n",
    "        m = resnet50()\n",
    "        # Extract 4 main layers (note: MaskRCNN needs this particular name\n",
    "        # mapping for return nodes)\n",
    "        self.body = create_feature_extractor(\n",
    "            m, return_nodes={f'layer{k}': str(v)\n",
    "                             for v, k in enumerate([1, 2, 3, 4])})\n",
    "        # Dry run to get number of channels for FPN\n",
    "        inp = torch.randn(2, 3, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            out = self.body(inp)\n",
    "        in_channels_list = [o.shape[1] for o in out.values()]\n",
    "        # Build FPN\n",
    "        self.out_channels = 256\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list, out_channels=self.out_channels,\n",
    "            extra_blocks=LastLevelMaxPool())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.fpn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Now we can build our model!\n",
    "model = MaskRCNN(Resnet50WithFPN(), num_classes=91).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3f94bb-1fa4-4b9b-a33f-bd53f9573a51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x =torch.rand(1, 3, 300, 300)\n",
    "md=model(x)\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc5883a-f2ac-416e-889a-fd94334eacb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('feat1', torch.Size([1, 64, 56, 56])), ('feat2', torch.Size([1, 256, 14, 14]))]\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.resnet18()\n",
    "# extract layer1 and layer3, giving as names `feat1` and feat2`\n",
    "model = create_feature_extractor(\n",
    "    model, {'layer1': 'feat1', 'layer3': 'feat2'})\n",
    "out = model(torch.rand(1, 3, 224, 224))\n",
    "print([(k, v.shape) for k, v in out.items()])\n",
    "\n",
    "\n",
    "# Specifying leaf modules and leaf functions\n",
    "def leaf_function(x):\n",
    "    # This would raise a TypeError if traced through\n",
    "    return int(x)\n",
    "\n",
    "class LeafModule(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        # This would raise a TypeError if traced through\n",
    "        int(x.shape[0])\n",
    "        return torch.nn.functional.relu(x + 4)\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(3, 1, 3)\n",
    "        self.leaf_module = LeafModule()\n",
    "\n",
    "    def forward(self, x):\n",
    "        leaf_function(x.shape[0])\n",
    "        return self.leaf_module(x)\n",
    "\n",
    "model = create_feature_extractor(\n",
    "    MyModule(), return_nodes=['leaf_module'],\n",
    "    tracer_kwargs={'leaf_modules': [LeafModule],\n",
    "                   'autowrap_functions': [leaf_function]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebff767b-10ca-4e11-809d-d14878c841be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "Wall time: 174 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch.nn as nn\n",
    "model = torchvision.models.resnet18()\n",
    "\n",
    "model_fc_num=model.fc.in_features\n",
    "model.fc=nn.Linear(model_fc_num,1024)\n",
    "sample=torch.rand(1, 3, 224, 224)\n",
    "\n",
    "result=model(sample)\n",
    "print(len(result[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ab5896-2b91-4707-8b27-6daef5744d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n",
      "Wall time: 175 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = torchvision.models.resnet18()\n",
    "dd=model(sample)\n",
    "print(dd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7b07b54f-88d9-4962-925e-6c32807611ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 56, 56])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = torchvision.models.resnet18()\n",
    "m.fc.out_features=1024\n",
    "model=create_feature_extractor(m,{'layer1': 'feat1'})\n",
    "r=model(sample)\n",
    "r.get('feat1').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "010ec79c-09e7-4502-83d1-a0510dad7f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([-0.3,.2,1,3,5])\n",
    "torch.clamp(a,0,1) #小于0设置为0，大于1，设置为1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e6bd6-4d21-433a-bec7-697b7eeeefc6",
   "metadata": {},
   "source": [
    "# 特征值提取与图片对比汇总："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f55dba0b-792d-46be-ad3b-f902ec4699d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img1_norm.shape: torch.Size([3, 500, 486])\n",
      "相关系数 tensor([[1.0000, 0.7635, 0.7635],\n",
      "        [0.7635, 1.0000, 1.0000],\n",
      "        [0.7635, 1.0000, 1.0000]], grad_fn=<ClampBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
    "from torchvision.models.detection.backbone_utils import LastLevelMaxPool\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "import torchvision\n",
    "import cv2 as cv\n",
    "#首次执行报错，因为路径下缺少对应文件，在报错提示的链接中，下载resnet18-f37072fd.pth，保存在报错路径即可\n",
    "model = torchvision.models.resnet18(pretrained=True) #设置Pretrained为True，意思为引用加载文件模型中的权重参数\n",
    "model_fc_num=model.fc.in_features\n",
    "model.fc=nn.Linear(model_fc_num,1000) #设置model的fc层，指定输出维度为1维tensor，里面含有1000向量\n",
    "\n",
    "#定义图像转换，将uint8转为0-1，\n",
    "transform_gy=torchvision.transforms.ToTensor() #将uint8转为0-1，即归一化，此步操作会变动图片通道，由[h,w,c]转为[c,h,w]\n",
    "transform_bz=torchvision.transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]) #数据标准化，均值，方差均为0.5\n",
    "\n",
    "transform_compose=torchvision.transforms.Compose([transform_gy,transform_bz])\n",
    "\n",
    "#下面用cv读取图片，并进行转换\n",
    "path1=r'D:\\pytorch\\VOCtrainval_11-May-2012\\VOCtrainval_11-May-2012\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000027.jpg'\n",
    "path2=r'D:\\pytorch\\VOCtrainval_11-May-2012\\VOCtrainval_11-May-2012\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000464.jpg'\n",
    "img1=cv.imdecode(np.fromfile(path1,dtype=np.uint8),cv.IMREAD_COLOR)\n",
    "img1_norm=transform_compose(img1)\n",
    "print('img1_norm.shape:',img1_norm.shape)\n",
    "# torch.stack([img1,img2]) Concatenates a sequence of tensors along a new dimension.\n",
    "img2=cv.imdecode(np.fromfile(path2,dtype=np.uint8),cv.IMREAD_COLOR)\n",
    "img2_norm=transform_compose(img2)\n",
    "\n",
    "#对图片进行转换，设置为n*c*h*w格式，以适model的输入要求\n",
    "im1=torch.stack([img1_norm])\n",
    "im2=torch.stack([img2_norm])\n",
    "\n",
    "model=model.eval() #设置为评估模式\n",
    "#调用模型，提取特征值\n",
    "fc1=model(im1) \n",
    "fc2=model(im2)\n",
    "\n",
    "fc=torch.stack([fc1[0].flatten(),fc2[0].flatten()])\n",
    "corrcoef=torch.corrcoef(fc)\n",
    "print(\"相关系数\",corrcoef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69efbe22-6b32-4de7-8b3b-1928dd4814ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6397, -0.3377,  0.3765,  ...,  0.5163, -0.5383, -0.5726],\n",
       "        [-0.4068, -0.3532,  0.1713,  ...,  0.6454, -0.2991, -0.3271]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9cf8b65-a5be-4b7a-9edb-f97ff07ec571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToTensor()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "907741b5-595b-460c-b203-f870c75872c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd11ea-ab04-4867-84da-e72853be1fc0",
   "metadata": {},
   "source": [
    "### 使用pytorch官网发布的resnet18模型参数，输出图像特征值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3219f96-80bc-4702-b54e-83a8d5b9c599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1779, -0.0292, -0.6425, -0.7108,  0.4744,  0.5333,  0.6039, -0.3617,\n",
       "          0.1788, -0.6468]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import cv2 as cv\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "#定义图像转换，将uint8转为0-1，\n",
    "transform_gy=torchvision.transforms.ToTensor() #将uint8转为0-1，即归一化，此步操作会变动图片通道，由[h,w,c]转为[c,h,w]\n",
    "transform_bz=torchvision.transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]) #数据标准化，均值，方差均为0.5\n",
    "transfrom_compose=torchvision.transforms.Compose([transform_gy,transform_bz])\n",
    "path1=r'D:\\pytorch\\VOCtrainval_11-May-2012\\VOCtrainval_11-May-2012\\VOCdevkit\\VOC2012\\JPEGImages\\2007_000027.jpg'\n",
    "resnet18=r'D:\\jupyterlab\\pytorch\\data\\resnet18-f37072fd.pth'\n",
    "img1=cv.imdecode(np.fromfile(path1,dtype=np.uint8),cv.IMREAD_COLOR)\n",
    "img1_norm=transfrom_compose(img1)\n",
    "model=torchvision.models.resnet18(pretrained=True)\n",
    "model.load_state_dict(torch.load(resnet18))\n",
    "model_fc_num=model.fc.in_features\n",
    "model.fc=nn.Linear(model_fc_num,10)\n",
    "\n",
    "model=model.eval()\n",
    "\n",
    "img1=cv.imdecode(np.fromfile(path1,dtype=np.uint8),cv.IMREAD_COLOR)\n",
    "img1=cv.resize(img1,(300,300))\n",
    "img1_norm=transfrom_compose(img1)\n",
    "img_tensor1=torch.Tensor(img1_norm)\n",
    "im1=torch.stack([img1_norm])\n",
    "model(im1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5001f7-2ce0-4e23-b690-ac18f460e161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
