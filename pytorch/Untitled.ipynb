{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88349683-c7b0-4d46-ae58-eb4871fe303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "from torch import nn,optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b05a0f-dc63-49c7-91da-665d30e6e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([60., 97.])\n",
      "tensor([-4.,  4.])\n"
     ]
    }
   ],
   "source": [
    "#model=torchvision.models.resnet18(pretrained=True)\n",
    "a=torch.tensor([2.,3.],requires_grad=True)\n",
    "b=torch.tensor([6.,4.],requires_grad=True)\n",
    "Q=3*a**3-b**2+2*a*b*2\n",
    "external_grad=torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad) #\n",
    "print(a.grad)#计算Q对a的偏导\n",
    "print(b.grad)#计算Q对b的偏导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c8bd632-839b-4a8d-b4a3-2a50df5fb88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399e8dd0-4e2f-4414-a099-f1c07f8d4b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "training_data=datasets.FashionMNIST(root='data',\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=ToTensor(),)\n",
    "test_data=datasets.FashionMNIST(root='data',\n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=ToTensor(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e195bbb-eac8-4743-9321-026c1034c944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x[N,C,H,W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y： torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "train_dataloader=DataLoader(training_data,batch_size=batch_size)  #每64个图片为一组，进行训练\n",
    "test_dataloader=DataLoader(test_data,batch_size=batch_size)\n",
    "for x,y in test_dataloader:\n",
    "    print('Shape of x[N,C,H,W]:',x.shape)\n",
    "    print('Shape of y：',y.shape,y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce19c164-0d82-4335-8967-bcdfbf820b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('using {} device'.format(device))\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork,self).__init__()  #super调用父类的方法，此次为nn.Module的方法\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "                                            nn.Linear(28*28,512),   #28*28：输入特征向量，512：输出特征向量\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(512,512),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(512,10))\n",
    "    def forward (self,x):\n",
    "        x=self.flatten(x)\n",
    "        logits=self.linear_relu_stack(x)   #linear_relu_stack为前7行定义的self.linear_relu_stack，用以对每个x执行linear_relu_stack函数的计算\n",
    "        return logits\n",
    "    \n",
    "model=NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86394606-2979-4737-8591-68d691ba7d28",
   "metadata": {},
   "source": [
    "# Optimizing the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd927a0-45b5-499e-8ec6-a3d3d4c05857",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b013cc8a-cd0c-416f-b2ee-a76460c88cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader,model,loss_fn,optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch,(x,y) in enumerate(dataloader):\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        \n",
    "        #compute prediction error\n",
    "        pred=model(x)\n",
    "        loss=loss_fn(pred,y)\n",
    "        \n",
    "        #backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch%100==0:\n",
    "            loss,current=loss.item(),batch*len(x)\n",
    "            print(f'loss:{loss:>7f} [{current:>5d}/{size :>5d}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dce9414-031e-4f0e-9c26-991fcc95ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader,model,loss_fn):\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches=len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss,correct=0,0\n",
    "    with torch.no_grad():\n",
    "        for x ,y in dataloader:\n",
    "            x,y=x.to(device),y.to(device)\n",
    "            pred=model(x)\n",
    "            test_loss_=loss_fn(pred,y).item()\n",
    "            correct +=(pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss/=num_batches\n",
    "    correct/=size\n",
    "    print(f'Test Error:\\n Accuracy:{(100*correct):>0.1f}%, Avg loss:{test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6006f55-d346-4471-b640-11268028a93c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      "--------------\n",
      "loss:2.309947 [    0/60000]\n",
      "loss:2.291602 [ 6400/60000]\n",
      "loss:2.272736 [12800/60000]\n",
      "loss:2.261524 [19200/60000]\n",
      "loss:2.253564 [25600/60000]\n",
      "loss:2.219537 [32000/60000]\n",
      "loss:2.231924 [38400/60000]\n",
      "loss:2.196790 [44800/60000]\n",
      "loss:2.200413 [51200/60000]\n",
      "loss:2.167365 [57600/60000]\n",
      "Test Error:\n",
      " Accuracy:46.9%, Avg loss:0.000000\n",
      "\n",
      "Epoch2\n",
      "--------------\n",
      "loss:2.176572 [    0/60000]\n",
      "loss:2.158749 [ 6400/60000]\n",
      "loss:2.104409 [12800/60000]\n",
      "loss:2.116866 [19200/60000]\n",
      "loss:2.079594 [25600/60000]\n",
      "loss:2.015186 [32000/60000]\n",
      "loss:2.050142 [38400/60000]\n",
      "loss:1.969695 [44800/60000]\n",
      "loss:1.978782 [51200/60000]\n",
      "loss:1.909867 [57600/60000]\n",
      "Test Error:\n",
      " Accuracy:59.5%, Avg loss:0.000000\n",
      "\n",
      "Epoch3\n",
      "--------------\n",
      "loss:1.940635 [    0/60000]\n",
      "loss:1.906246 [ 6400/60000]\n",
      "loss:1.790090 [12800/60000]\n",
      "loss:1.826541 [19200/60000]\n",
      "loss:1.734727 [25600/60000]\n",
      "loss:1.673314 [32000/60000]\n",
      "loss:1.706450 [38400/60000]\n",
      "loss:1.597343 [44800/60000]\n",
      "loss:1.631254 [51200/60000]\n",
      "loss:1.520404 [57600/60000]\n",
      "Test Error:\n",
      " Accuracy:61.0%, Avg loss:0.000000\n",
      "\n",
      "Epoch4\n",
      "--------------\n",
      "loss:1.604844 [    0/60000]\n",
      "loss:1.561270 [ 6400/60000]\n",
      "loss:1.409194 [12800/60000]\n",
      "loss:1.478168 [19200/60000]\n",
      "loss:1.369245 [25600/60000]\n",
      "loss:1.357435 [32000/60000]\n",
      "loss:1.383262 [38400/60000]\n",
      "loss:1.294240 [44800/60000]\n",
      "loss:1.339921 [51200/60000]\n",
      "loss:1.232477 [57600/60000]\n",
      "Test Error:\n",
      " Accuracy:63.0%, Avg loss:0.000000\n",
      "\n",
      "Epoch5\n",
      "--------------\n",
      "loss:1.338100 [    0/60000]\n",
      "loss:1.309972 [ 6400/60000]\n",
      "loss:1.145814 [12800/60000]\n",
      "loss:1.246811 [19200/60000]\n",
      "loss:1.130440 [25600/60000]\n",
      "loss:1.156114 [32000/60000]\n",
      "loss:1.184764 [38400/60000]\n",
      "loss:1.109076 [44800/60000]\n",
      "loss:1.155204 [51200/60000]\n",
      "loss:1.066700 [57600/60000]\n",
      "Test Error:\n",
      " Accuracy:64.6%, Avg loss:0.000000\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch{t+1}\\n--------------')\n",
    "    train(train_dataloader,model,loss_fn,optimizer)\n",
    "    test(test_dataloader,model,loss_fn)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed4536-ba71-45d3-b349-a472f3075353",
   "metadata": {},
   "source": [
    "# save models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e81b5814-5566-42b1-9133-56911b6191f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Pytorch Model State to model.path\n",
      "model's Parameters OrderedDict([('linear_relu_stack.0.weight', tensor([[ 0.0297, -0.0158, -0.0289,  ..., -0.0288, -0.0197,  0.0142],\n",
      "        [-0.0084,  0.0065, -0.0120,  ..., -0.0133, -0.0108,  0.0124],\n",
      "        [-0.0010, -0.0313, -0.0148,  ..., -0.0247, -0.0208,  0.0346],\n",
      "        ...,\n",
      "        [ 0.0272,  0.0235, -0.0144,  ...,  0.0215, -0.0350,  0.0063],\n",
      "        [-0.0310, -0.0343,  0.0075,  ...,  0.0189,  0.0085,  0.0152],\n",
      "        [-0.0135,  0.0090,  0.0074,  ...,  0.0264,  0.0356,  0.0026]],\n",
      "       device='cuda:0')), ('linear_relu_stack.0.bias', tensor([-3.2084e-03, -2.6568e-02, -2.4015e-02,  1.9518e-02,  2.6756e-02,\n",
      "         1.2515e-02,  2.0961e-02, -3.2916e-02, -2.2673e-02, -2.6601e-02,\n",
      "        -1.8379e-02,  4.0674e-02,  1.6384e-02, -1.0032e-02, -9.0005e-03,\n",
      "        -4.2968e-03,  3.1889e-02,  2.9030e-03, -1.4166e-02, -1.1304e-02,\n",
      "        -1.8850e-02, -3.4805e-02, -2.6738e-02,  1.7201e-02, -1.4999e-02,\n",
      "         3.4140e-02, -3.3903e-02,  1.1548e-02,  8.3324e-04, -1.6668e-02,\n",
      "         3.8087e-02, -2.4879e-02,  2.7325e-02, -2.9087e-02,  6.8579e-03,\n",
      "         2.9881e-02,  3.7049e-02, -1.8706e-02, -2.0022e-02,  3.2324e-03,\n",
      "        -2.9041e-03,  1.2614e-02, -2.8753e-02,  9.0030e-03, -1.5779e-04,\n",
      "        -7.1774e-03, -8.7404e-05,  2.3790e-02, -1.4102e-02, -1.0631e-02,\n",
      "         1.9345e-02, -2.0337e-02,  3.9554e-03, -4.4923e-03,  1.0763e-03,\n",
      "         1.7934e-02,  1.1875e-02, -5.7500e-03, -6.4691e-03, -6.1607e-03,\n",
      "         1.9587e-02, -2.9020e-02,  2.9579e-02, -2.6253e-02, -9.1242e-03,\n",
      "         3.2701e-02, -3.2385e-02,  2.5548e-03, -1.6450e-02, -2.7849e-02,\n",
      "         4.6264e-03,  3.1085e-03, -2.7384e-02, -9.9366e-03, -2.4397e-02,\n",
      "         2.5351e-02,  1.2061e-02, -2.9580e-03, -2.5715e-02,  3.1487e-02,\n",
      "        -3.4098e-02,  3.4871e-02,  2.0630e-02,  2.2681e-02,  1.1982e-02,\n",
      "        -3.0093e-02, -6.3365e-03, -7.8224e-03, -1.7830e-02,  2.4551e-02,\n",
      "        -1.9387e-02, -1.9115e-02,  2.1038e-02, -2.3443e-02, -1.8618e-02,\n",
      "        -4.9560e-04, -2.6004e-02, -7.6364e-03,  2.3851e-02,  1.4995e-02,\n",
      "        -1.0996e-02,  1.0035e-02,  3.4742e-02,  2.6577e-02,  3.5607e-02,\n",
      "         2.9789e-03,  3.4109e-02,  6.5585e-03,  6.4629e-03,  1.8423e-02,\n",
      "        -3.2349e-02, -3.1068e-02,  1.9767e-02,  1.8665e-02, -2.0457e-02,\n",
      "         3.0649e-02,  2.9790e-02, -2.8960e-02,  3.3685e-02,  2.9344e-02,\n",
      "         2.5322e-02,  2.7872e-02, -2.4063e-02, -2.5354e-02, -6.4621e-03,\n",
      "         3.1159e-02, -1.6739e-02,  1.9193e-02,  3.4884e-02,  2.3538e-02,\n",
      "         2.7340e-02,  3.1235e-02,  2.4419e-02, -3.0274e-02,  6.3683e-03,\n",
      "        -2.4832e-02, -1.8618e-02,  3.1386e-03, -3.4204e-02, -4.6527e-03,\n",
      "        -2.2786e-02, -3.0302e-02,  2.3901e-02,  8.8691e-04, -2.1716e-02,\n",
      "        -8.0532e-03, -5.3037e-03, -2.1344e-02,  2.5051e-03, -1.8322e-02,\n",
      "         4.6908e-04,  2.1750e-02,  3.3207e-03, -2.4983e-02,  3.5732e-02,\n",
      "         1.3827e-02,  1.0240e-02, -1.5097e-02,  6.6507e-03, -4.7990e-04,\n",
      "        -2.0975e-02,  2.9727e-02,  6.1625e-03, -2.0969e-02, -2.1594e-02,\n",
      "        -4.1695e-03,  2.1671e-02, -6.3389e-03,  4.7746e-03, -2.7290e-02,\n",
      "         2.9901e-02,  2.2405e-02, -2.0436e-03, -1.5620e-02, -1.6576e-02,\n",
      "         1.1988e-02,  3.0470e-02,  2.5371e-02, -4.3483e-03,  1.4780e-02,\n",
      "        -4.4447e-03, -2.4505e-02,  2.8890e-02, -5.4120e-03,  3.8823e-03,\n",
      "         2.3127e-02,  1.9268e-02,  1.3327e-02,  4.5544e-02, -2.0439e-03,\n",
      "         2.1291e-02,  2.9291e-02,  1.4002e-02, -2.4083e-02,  5.6609e-03,\n",
      "        -2.0427e-02, -4.1408e-03, -3.2341e-02,  4.8459e-03,  1.0992e-02,\n",
      "         2.2969e-03,  1.7206e-02,  5.8979e-03, -3.3109e-02, -7.6866e-03,\n",
      "        -7.4034e-03,  1.4195e-02, -1.9971e-02,  1.4938e-02, -1.8279e-02,\n",
      "         3.9926e-02,  2.8837e-02, -2.1854e-02, -2.8368e-02, -2.0497e-02,\n",
      "         1.5917e-02, -1.2164e-02, -3.4115e-02,  1.9313e-02, -5.2779e-04,\n",
      "         7.3607e-03,  3.8667e-02, -2.2500e-03, -2.6048e-02, -1.3052e-03,\n",
      "         1.2191e-02, -7.3289e-03,  2.0543e-02, -2.7027e-02, -1.4596e-02,\n",
      "         1.6064e-02,  3.2046e-02,  1.7603e-02,  1.7175e-02, -7.1080e-03,\n",
      "        -4.6115e-03,  3.3746e-02,  2.0892e-02, -3.5178e-02, -2.3602e-02,\n",
      "        -1.2015e-02,  3.3334e-02,  1.0740e-03, -2.4323e-02,  2.2189e-02,\n",
      "         2.2342e-02, -6.5745e-03, -2.1608e-02,  1.7691e-02,  3.1667e-02,\n",
      "        -8.1687e-03,  9.8281e-03, -1.9551e-02,  3.1297e-02, -2.7940e-02,\n",
      "         1.3733e-02,  2.2181e-02,  2.8333e-02, -2.3134e-02, -2.0877e-02,\n",
      "        -2.1977e-03,  2.6506e-02,  3.1761e-02,  4.0276e-02,  3.8328e-02,\n",
      "         1.6174e-02, -2.1716e-02, -3.1995e-02, -3.2838e-02, -5.7724e-03,\n",
      "        -2.2254e-02, -2.1513e-02,  2.3444e-02,  6.2451e-04, -1.9767e-02,\n",
      "         2.6745e-02, -1.9109e-02,  1.4732e-02,  1.9326e-03, -2.2425e-02,\n",
      "        -3.2314e-02,  2.8251e-02,  4.3253e-03,  1.1706e-02,  2.6500e-02,\n",
      "        -2.7924e-02,  2.6034e-02, -2.3030e-02,  1.2309e-02,  1.4675e-02,\n",
      "        -1.9179e-02,  1.1066e-02, -2.6345e-02, -2.9279e-04, -2.6305e-03,\n",
      "        -3.9232e-03, -6.4145e-03,  1.2838e-02,  2.5821e-03, -2.7012e-02,\n",
      "         7.8815e-04,  3.0849e-02,  3.4951e-03,  2.3243e-02,  1.4138e-02,\n",
      "        -2.8692e-02, -2.4570e-02,  4.2293e-02,  2.7641e-02,  7.7425e-03,\n",
      "         4.1477e-02, -2.9383e-03,  2.9395e-02,  1.1653e-02,  1.9242e-02,\n",
      "         8.9685e-03,  1.0282e-02, -2.3543e-02,  2.8109e-02, -7.7465e-03,\n",
      "         2.5019e-02, -2.9842e-02,  1.0317e-02,  3.2147e-03,  3.7133e-02,\n",
      "        -2.0352e-02, -1.9241e-02,  2.3650e-02,  2.2125e-02,  3.2151e-02,\n",
      "         3.1268e-02, -1.3279e-02, -2.7588e-02, -2.6233e-02,  1.6878e-02,\n",
      "         1.9497e-03,  3.0820e-02,  1.9612e-02, -1.1775e-03, -4.9616e-03,\n",
      "        -1.4883e-02, -1.7023e-04, -2.9071e-03, -1.0177e-02, -1.0625e-02,\n",
      "        -3.0437e-02, -3.4980e-02, -1.6168e-02,  3.4425e-02,  1.7475e-03,\n",
      "         8.2626e-03,  9.0176e-03, -8.2770e-04,  3.8333e-02, -1.6005e-02,\n",
      "         1.9644e-02,  2.7024e-02,  2.6238e-02,  1.8429e-02, -1.9035e-02,\n",
      "        -4.7024e-03, -2.1340e-02, -4.9886e-03, -5.1997e-03,  2.6204e-02,\n",
      "         3.3862e-02, -8.8733e-03,  3.7026e-02, -7.5387e-04, -2.8229e-02,\n",
      "         6.6041e-03, -1.2064e-02, -5.3073e-05,  1.6718e-02, -3.4651e-02,\n",
      "         3.7341e-04,  2.4503e-02,  1.5378e-03, -2.1636e-04,  3.1068e-02,\n",
      "        -6.3853e-03, -2.6756e-02, -1.7076e-02,  2.8963e-02,  1.9321e-02,\n",
      "        -2.5058e-02, -1.0968e-02, -2.1049e-02,  2.6837e-02,  3.0679e-02,\n",
      "        -3.1966e-02, -8.9642e-03,  9.9293e-03,  2.9789e-02,  3.2142e-03,\n",
      "        -1.8542e-03,  1.8474e-02,  2.1941e-02,  1.5086e-02, -1.4919e-02,\n",
      "         7.1294e-03,  3.1314e-02, -3.1800e-02,  3.1540e-02,  2.3483e-02,\n",
      "         6.5966e-03,  2.4863e-02,  1.4131e-02, -4.1810e-03, -4.0013e-03,\n",
      "         2.9850e-02, -2.0572e-02,  3.7203e-04, -3.4515e-02,  2.4894e-02,\n",
      "         1.3064e-02, -1.4499e-03,  2.6328e-02,  3.6564e-02,  2.2144e-02,\n",
      "         1.0217e-02,  3.4294e-02,  1.8864e-02, -3.7926e-03,  1.4380e-02,\n",
      "         2.4946e-02,  3.0632e-02, -1.9000e-02, -2.0407e-02, -3.1952e-02,\n",
      "         1.9063e-02, -2.3753e-02, -9.8438e-03,  4.0798e-02, -2.6847e-02,\n",
      "         3.8295e-02, -9.4437e-03,  2.0222e-02, -2.2709e-02, -5.0974e-03,\n",
      "        -9.5388e-03, -8.9952e-03, -2.7583e-02,  2.5716e-02,  3.5878e-02,\n",
      "        -6.3162e-03, -1.3396e-02,  1.5568e-02,  1.6074e-02,  1.7909e-02,\n",
      "        -1.5082e-02,  1.6311e-02, -2.9143e-02, -1.6747e-03, -3.7758e-03,\n",
      "         2.4651e-02, -3.2719e-02,  9.9031e-03, -1.9491e-02, -1.8027e-02,\n",
      "         2.2671e-02,  1.4821e-03,  1.4449e-02, -1.2824e-03, -2.5856e-03,\n",
      "        -2.4057e-02, -1.6520e-03, -1.9872e-02, -1.8184e-02,  2.4894e-02,\n",
      "         1.3016e-02,  1.1277e-02,  1.1401e-02,  3.2856e-02, -2.6208e-02,\n",
      "         1.3784e-02, -1.8282e-02,  3.5775e-02, -2.1546e-02,  9.7226e-03,\n",
      "         1.5335e-02, -6.6970e-03,  3.5996e-02,  1.7113e-02,  2.3260e-02,\n",
      "         9.9897e-04,  1.0767e-02, -1.8407e-02,  3.9549e-02, -1.5598e-02,\n",
      "        -1.6646e-02, -3.2443e-02, -3.0354e-02,  3.7146e-02, -1.8698e-02,\n",
      "        -2.9060e-02, -3.4431e-02, -1.9928e-02, -3.2645e-03,  2.8602e-04,\n",
      "         7.0032e-03,  3.3703e-02,  1.1098e-03,  2.4148e-02,  1.8691e-02,\n",
      "         1.9717e-02, -1.3829e-02, -1.9041e-02,  2.9607e-02, -2.9477e-02,\n",
      "         4.2769e-03,  2.1412e-02], device='cuda:0')), ('linear_relu_stack.2.weight', tensor([[ 0.0225, -0.0108,  0.0102,  ..., -0.0226,  0.0330, -0.0067],\n",
      "        [-0.0255,  0.0139, -0.0296,  ...,  0.0389,  0.0138, -0.0198],\n",
      "        [ 0.0387,  0.0287, -0.0275,  ...,  0.0338,  0.0124, -0.0429],\n",
      "        ...,\n",
      "        [ 0.0406, -0.0350, -0.0259,  ..., -0.0084,  0.0224, -0.0075],\n",
      "        [ 0.0232, -0.0099, -0.0089,  ...,  0.0310, -0.0382, -0.0148],\n",
      "        [-0.0148,  0.0194,  0.0281,  ..., -0.0188, -0.0119, -0.0281]],\n",
      "       device='cuda:0')), ('linear_relu_stack.2.bias', tensor([ 8.7867e-03,  1.9754e-02,  2.1553e-02,  4.3128e-02, -4.2342e-02,\n",
      "         1.0247e-02, -2.1680e-02, -3.8509e-02, -3.0974e-03, -1.5629e-03,\n",
      "        -3.5703e-02,  2.8686e-02, -8.8994e-03,  3.6448e-02, -3.0564e-02,\n",
      "         2.6114e-02,  2.0992e-02, -3.2723e-03, -1.2861e-02,  1.3083e-02,\n",
      "         2.0932e-02, -9.2042e-03, -3.3681e-02, -1.6389e-02,  3.6357e-02,\n",
      "         6.1988e-02,  4.8097e-02, -4.5765e-04, -4.1387e-02, -2.5724e-02,\n",
      "         7.6291e-03,  1.3680e-03,  3.9354e-02,  3.3301e-02,  4.2854e-02,\n",
      "        -2.7151e-02, -4.4339e-03,  3.8816e-02,  2.0400e-02,  2.0313e-02,\n",
      "         1.0567e-02,  1.2204e-02,  1.3032e-02, -1.9086e-02, -3.6470e-02,\n",
      "        -3.5650e-02, -3.6440e-02, -1.4297e-02,  7.8539e-03, -2.0834e-02,\n",
      "        -3.7711e-03,  8.9500e-03,  2.2545e-02, -2.6333e-02,  9.4960e-03,\n",
      "        -2.9256e-02,  4.7228e-02, -4.8481e-03,  3.9403e-02,  2.9869e-02,\n",
      "         2.0636e-02,  2.1230e-02, -2.3780e-02,  1.6629e-02, -1.1094e-02,\n",
      "         1.9537e-02,  1.7968e-02,  3.8442e-02,  2.8775e-02, -2.4598e-02,\n",
      "         2.3863e-02,  1.7608e-02, -3.1103e-02, -1.5523e-03, -9.3748e-03,\n",
      "        -2.5475e-02,  3.5363e-02, -5.5347e-04, -4.0325e-02, -4.0537e-03,\n",
      "        -3.9590e-02,  2.2376e-03, -2.8610e-02, -4.8860e-03,  3.5941e-02,\n",
      "         3.5434e-02,  4.6805e-03, -1.6589e-02, -3.5264e-02,  5.5060e-03,\n",
      "         1.4855e-02, -5.0196e-02,  3.0603e-02, -9.7297e-03,  3.7862e-02,\n",
      "        -5.6770e-03, -7.7654e-03, -3.9543e-02,  3.2445e-02,  5.2449e-02,\n",
      "        -6.7122e-03,  2.6634e-02,  8.1089e-04, -3.2411e-02,  3.8050e-02,\n",
      "         1.3484e-02, -3.8750e-02,  3.2173e-02, -1.2508e-02,  4.9316e-02,\n",
      "         2.2044e-02, -3.7799e-02,  5.3298e-02,  4.3901e-02,  1.5300e-02,\n",
      "        -3.9087e-02, -2.3422e-02,  5.0378e-02,  1.1652e-02, -2.8126e-02,\n",
      "        -3.2422e-02, -2.4196e-02, -1.0566e-02,  1.1722e-02,  2.9320e-02,\n",
      "         4.4475e-02, -2.3365e-02, -8.5749e-03, -2.6007e-02, -6.9446e-03,\n",
      "        -4.1126e-02, -4.4785e-03, -6.4170e-03, -1.5152e-02,  7.2637e-03,\n",
      "         5.4093e-05,  8.9070e-03,  2.3473e-02,  2.8347e-03,  1.4890e-02,\n",
      "         1.7079e-02,  1.7367e-03,  4.0987e-02,  2.4758e-02,  1.1972e-02,\n",
      "         2.2291e-02,  2.9263e-03,  3.8228e-02,  2.3364e-02,  3.8056e-02,\n",
      "        -1.3753e-02, -1.8423e-02,  3.5684e-02, -3.5169e-03, -4.2092e-02,\n",
      "         4.2065e-02, -2.9909e-02, -1.4970e-03,  4.4446e-02, -6.5054e-03,\n",
      "        -1.8284e-02,  7.0856e-04, -1.5332e-02,  4.2104e-02,  1.4212e-02,\n",
      "        -3.5133e-02,  1.2604e-02, -3.3888e-02,  2.1723e-02, -1.1640e-02,\n",
      "         4.5666e-03,  3.2975e-02,  3.9761e-02,  5.7629e-02,  5.6168e-03,\n",
      "        -4.7791e-03,  3.0670e-02, -2.5160e-02, -3.1443e-02, -3.7470e-03,\n",
      "        -1.7293e-02, -9.8752e-03,  3.7435e-02, -2.1394e-02, -2.6733e-02,\n",
      "         3.8686e-02, -1.7790e-02,  4.2994e-03, -1.3318e-02,  4.7272e-02,\n",
      "        -2.9786e-02, -3.2903e-02,  1.8788e-02,  3.4224e-02,  2.1973e-03,\n",
      "         4.3165e-02, -2.2803e-02,  3.2833e-02,  8.3060e-03, -5.4198e-03,\n",
      "         8.6300e-03,  3.5923e-02,  1.8297e-02,  6.9790e-03,  1.8425e-02,\n",
      "         1.9141e-02,  4.5784e-02, -2.6704e-02, -1.0137e-02, -5.8801e-03,\n",
      "         1.0515e-02,  3.5731e-02,  1.2356e-03,  3.0510e-02,  1.1975e-02,\n",
      "         3.5399e-02, -3.3877e-02,  1.7788e-02,  1.3705e-02, -1.7129e-02,\n",
      "        -2.9169e-02, -1.0578e-02, -1.3266e-02,  4.0207e-02,  2.9324e-02,\n",
      "         4.9593e-02,  3.7537e-02,  3.7385e-02, -3.8298e-02, -5.9167e-03,\n",
      "         3.3767e-02, -2.5585e-02,  3.7823e-02, -2.2232e-02,  2.9078e-02,\n",
      "         4.4790e-02,  2.8892e-02,  1.0224e-02,  7.8309e-03, -2.9664e-02,\n",
      "         3.2211e-02, -1.3084e-02,  3.6147e-02,  4.5468e-03,  3.4925e-03,\n",
      "        -1.6590e-02,  4.9529e-02,  4.0536e-02,  1.1945e-02, -2.0648e-02,\n",
      "        -1.5894e-02, -8.5758e-03, -3.2346e-03,  3.6706e-02, -2.4215e-02,\n",
      "         1.7177e-02, -1.7577e-02,  1.9394e-03,  3.7698e-02, -2.0310e-02,\n",
      "         3.7238e-02, -3.6174e-02,  1.6943e-02,  3.2898e-03,  4.4375e-03,\n",
      "        -1.8995e-02, -5.6482e-03, -8.9274e-03, -1.7805e-02,  3.7602e-03,\n",
      "         1.3055e-02, -1.2231e-02,  2.3547e-02, -3.8353e-02,  2.1264e-03,\n",
      "        -3.1447e-02, -1.0323e-02, -1.2677e-02,  2.7262e-02,  2.0537e-02,\n",
      "         1.3869e-02,  3.5088e-02, -4.3305e-02, -3.5975e-02, -5.4323e-03,\n",
      "         1.1170e-02,  2.6323e-02, -1.5590e-03,  4.3507e-02,  1.2410e-03,\n",
      "        -2.9381e-02, -1.8303e-02,  2.4581e-02,  3.5220e-02, -3.6785e-02,\n",
      "         3.8362e-02, -1.5184e-02, -1.9504e-02, -2.2267e-02,  2.4615e-02,\n",
      "         1.0178e-02,  2.0759e-03, -1.9389e-02, -9.6929e-03,  7.1959e-03,\n",
      "        -2.7908e-02,  4.8674e-02,  1.0471e-03,  3.9431e-02, -3.5847e-02,\n",
      "        -8.6580e-03,  2.0435e-02,  2.1169e-02,  3.4128e-02,  1.1755e-02,\n",
      "        -1.0709e-02,  3.5174e-02,  2.7878e-02,  1.2252e-02, -4.6306e-02,\n",
      "        -3.1072e-02, -2.3492e-02,  3.1181e-02, -7.0360e-03, -2.8971e-02,\n",
      "        -2.5885e-02,  2.1707e-02, -2.4948e-02,  4.9145e-03,  4.3146e-03,\n",
      "         3.8052e-02, -3.7673e-02, -2.0013e-02,  3.9830e-02,  2.9871e-02,\n",
      "         2.8981e-02,  2.9011e-02, -1.5909e-02, -1.1714e-02,  3.9238e-02,\n",
      "         2.2205e-02, -2.1127e-02, -1.3818e-03,  4.0286e-02, -7.7292e-03,\n",
      "        -4.0017e-02,  3.4810e-02,  2.4015e-02,  2.5205e-02, -3.5670e-02,\n",
      "        -3.5322e-02, -1.3758e-02, -2.0532e-02,  3.5707e-03,  2.6793e-02,\n",
      "        -3.3584e-02,  1.5625e-02,  4.0635e-02,  4.7751e-02, -6.6656e-03,\n",
      "        -2.3100e-02,  1.2391e-02,  4.0008e-03,  2.2719e-02,  5.2962e-02,\n",
      "        -4.1080e-02, -3.6466e-03,  1.1894e-02,  1.0935e-02, -3.4180e-04,\n",
      "         3.8510e-02,  1.1770e-02,  6.7703e-03, -3.6478e-03, -1.7268e-02,\n",
      "        -3.0854e-02,  2.5410e-02, -2.4582e-02, -2.1990e-02,  5.4325e-03,\n",
      "        -7.3350e-03,  6.9033e-03, -3.4443e-02, -2.5214e-02,  3.3655e-02,\n",
      "         2.4866e-02,  9.3786e-03, -8.4614e-03,  1.1400e-02, -1.6971e-02,\n",
      "        -4.4400e-02,  2.0907e-02,  1.3149e-02,  3.5917e-02, -1.5486e-02,\n",
      "         2.3666e-02, -3.2574e-02,  4.1483e-02, -2.3007e-02, -9.7712e-05,\n",
      "         2.7572e-02,  2.2084e-02, -2.0309e-02,  7.2068e-03, -2.5513e-02,\n",
      "        -1.9866e-02, -2.2336e-02, -1.8453e-02, -4.3045e-03, -2.5294e-02,\n",
      "        -4.5675e-02, -2.7942e-02,  3.7266e-02, -1.6373e-02, -1.5695e-02,\n",
      "        -4.3062e-03,  3.1227e-02, -3.8489e-02, -3.4045e-02, -3.3664e-02,\n",
      "         3.4447e-02,  2.5235e-03,  8.8241e-03,  3.2627e-02, -3.9863e-02,\n",
      "         9.9830e-03,  4.1692e-02, -3.4267e-02, -1.8532e-02, -1.8660e-03,\n",
      "        -2.8342e-02, -1.1821e-02, -3.8962e-02, -1.9837e-02,  4.1496e-02,\n",
      "        -2.9452e-02, -1.3878e-02,  2.2914e-02, -1.8137e-02,  3.7416e-02,\n",
      "         2.9224e-02,  2.3174e-03, -5.7404e-03, -1.0926e-02,  4.5576e-02,\n",
      "         2.5665e-02,  1.2760e-02,  2.3697e-03, -1.3586e-02,  1.7498e-02,\n",
      "        -2.3688e-02, -1.4226e-02,  2.2786e-02, -1.0674e-02, -6.6801e-03,\n",
      "        -2.4792e-02, -3.3701e-02,  8.6078e-03,  3.2356e-02, -1.4036e-02,\n",
      "         3.1103e-03,  3.0431e-02, -7.6851e-03,  2.6170e-02,  4.4609e-02,\n",
      "         5.4136e-02, -3.8607e-02, -1.4386e-02, -1.0316e-02,  4.3433e-02,\n",
      "         2.1862e-02, -3.3263e-02, -5.1279e-03, -3.4883e-02, -4.6902e-02,\n",
      "         3.3021e-02,  1.0212e-02,  3.7765e-02,  4.1548e-03, -4.0163e-02,\n",
      "        -3.5639e-02, -2.3034e-02,  4.7925e-03, -3.5217e-02,  2.8395e-02,\n",
      "        -1.0155e-02,  1.9094e-02, -3.6549e-02,  4.3959e-02,  2.0909e-02,\n",
      "         3.3032e-02,  1.0849e-03,  6.4122e-03, -1.5595e-02, -3.2855e-02,\n",
      "         1.7799e-02,  5.8964e-02,  6.9457e-03,  1.0112e-02, -3.1389e-02,\n",
      "         1.0959e-02, -1.2343e-02,  4.7675e-03,  4.1191e-02,  2.0429e-02,\n",
      "         2.1932e-02,  3.8441e-02, -3.2168e-03,  4.3095e-02,  2.8073e-02,\n",
      "        -3.4231e-02, -3.9324e-02], device='cuda:0')), ('linear_relu_stack.4.weight', tensor([[-0.0016,  0.0385, -0.0119,  ...,  0.0049,  0.0261, -0.0715],\n",
      "        [ 0.0264,  0.0229,  0.0166,  ..., -0.0271,  0.0093,  0.0095],\n",
      "        [ 0.0303, -0.0267, -0.0190,  ..., -0.0374, -0.0298, -0.0450],\n",
      "        ...,\n",
      "        [-0.0066, -0.0362,  0.0213,  ...,  0.0381, -0.0557,  0.0156],\n",
      "        [-0.0319, -0.0048, -0.0061,  ...,  0.0710,  0.0304,  0.0525],\n",
      "        [-0.0266,  0.0805, -0.0335,  ..., -0.0131,  0.0327,  0.0098]],\n",
      "       device='cuda:0')), ('linear_relu_stack.4.bias', tensor([-0.0743,  0.0209, -0.0221, -0.0430, -0.0542,  0.1413, -0.0126,  0.0776,\n",
      "        -0.0047, -0.0265], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),\"D:\\model.pth\")\n",
    "print(\"Saved Pytorch Model State to model.pth\")\n",
    "print(\"model's Parameters\",model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90116f4f-d651-4b27-a218-79c99f611151",
   "metadata": {},
   "source": [
    "# Loading Models:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f164213d-0f0e-425f-a966-9b2409552bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=NeuralNetwork()\n",
    "model.load_state_dict(torch.load(r\"D:\\model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94f023c-5f6e-47c5-9b68-f2680dc115f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\"Ankle boot\",Actual:\"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "#this model can now be used to make predictions\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "model.eval()\n",
    "x,y=test_data[0][0],test_data[0][1]  #x为test_data的数据值，y为test_data的真实标签索引\n",
    "with torch.no_grad():\n",
    "    pred=model(x)    #pred为各个标签预测的权重\n",
    "    predicted,actual=classes[pred[0].argmax(0)],classes[y]\n",
    "    print(f'Predicted:\"{predicted}\",Actual:\"{actual}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6fe06e-ae5b-4e83-83e1-3c6bc08747f1",
   "metadata": {},
   "source": [
    "# 核心方法&函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab20eb-26dc-416f-a06f-175e15201073",
   "metadata": {},
   "source": [
    "### ToTensor(): converts PIL image or Numpy ndarray into a FloatTensor.and scales the image's pixel intensity values in the range[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355ec2b-f592-474e-b881-891d3585a722",
   "metadata": {},
   "source": [
    "### transforms: to perform some manipulation of the data and make it suitable for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdfa4489-cb24-4e28-afe0-4bfffb2c750a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Lambda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14652/2400530318.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtarget_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#lambda:wo define a function to turn the integer into a one-hot encoded tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# It first creates a zero tensor of size 10 (the number of labels in our dataset) and call scatter_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# whch assigns a value=1 on the index as given by the label y.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Lambda' is not defined"
     ]
    }
   ],
   "source": [
    "target_transform = Lambda(lambda y: torch.tensor([1,3,4], dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "#lambda:wo define a function to turn the integer into a one-hot encoded tensor.\n",
    "# It first creates a zero tensor of size 10 (the number of labels in our dataset) and call scatter_ \n",
    "# whch assigns a value=1 on the index as given by the label y.\n",
    "print(target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8326ee35-b4a8-4de3-b156-aae87a2012d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "#调用PennFudanDataset类，并传入参数，则参数对应__init__中的参数，并执行__getitem__函数\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])       \n",
    "        img = Image.open(img_path).convert(\"RGB\") #读取路径下的一张图片，并转化为RGB\n",
    "        \n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)     #mask一共有三个元素构成[0,1,2]\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None] #分别提取两个objcet的mask\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i]) #返回mask非零坐标，结果为2个（x\\y轴）数组，数组组合对应非零元素的坐标位置，也可用c..nonzero()替代\n",
    "                                        # (array([170, 170, 170, ..., 485, 485, 485], dtype=int64),\n",
    "                                        #  array([452, 453, 454, ..., 514, 515, 516], dtype=int64))\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])  #矩形box的面积，长*宽\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        else:\n",
    "            print(\"transforms:\",self.transforms)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        print(\"-\"*10)\n",
    "        return len(self.imgs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e7708-9560-4c38-996b-ab31255575e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eda7a87-fcf4-4c07-9a14-dba05f9b7642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforms: None\n",
      "transforms: None\n",
      "(<PIL.Image.Image image mode=RGB size=559x536 at 0x16F68FD76D0>, {'boxes': tensor([[159., 181., 301., 430.],\n",
      "        [419., 170., 534., 485.]]), 'labels': tensor([1, 1]), 'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8), 'image_id': tensor([0]), 'area': tensor([35358., 36225.]), 'iscrowd': tensor([0, 0])})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot  as plt\n",
    "dataset=PennFudanDataset(r'D:\\pytorch\\pytorch_data\\PennFudanPed')\n",
    "img1=dataset[0][1].get('masks')[1]\n",
    "print(dataset[0])\n",
    "plt.imshow(img1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f94b1a1a-326d-4afc-a833-f31aa6f6c6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 397, 396])\n"
     ]
    }
   ],
   "source": [
    "dataset_test = PennFudanDataset('D:\\pytorch\\pytorch_data\\PennFudanPed', get_transform(train=False))\n",
    "x=dataset_test[3][0]\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fe5cd5a-dd28-4aea-a908-d4a954a7ffb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[163.5314,  57.3043, 333.0993, 345.9075],\n",
       "          [  2.2024,  56.7011,  51.9905, 184.9250],\n",
       "          [219.3185, 123.5419, 276.2725, 208.9558],\n",
       "          [191.9788, 193.0892, 209.3468, 204.1594],\n",
       "          [  0.0000,  67.8323,  22.7391, 117.5702],\n",
       "          [  0.6268,  81.4212,  32.2525, 142.0008],\n",
       "          [244.1865, 127.4016, 274.8847, 200.6274],\n",
       "          [ 38.9278, 132.7908,  55.8332, 159.1131],\n",
       "          [  0.7754, 108.8801,  30.4010, 136.4991],\n",
       "          [  0.0000,  66.9752,  31.5562, 153.5544],\n",
       "          [  0.0000, 111.9863,  35.9750, 138.3170],\n",
       "          [213.6364, 227.7622, 240.5092, 315.8253],\n",
       "          [190.9017,  59.3197, 279.7098, 268.4198],\n",
       "          [  1.5163, 141.4532,  38.3521, 184.6786],\n",
       "          [ 15.5272,  78.7711,  39.7515, 135.6513],\n",
       "          [227.6509, 230.4300, 240.1092, 245.4130],\n",
       "          [  9.7646,  59.8254,  47.6173, 132.4996],\n",
       "          [  2.4809,  79.4151,  26.7633, 117.7387],\n",
       "          [  2.7817,  80.7238,  26.0812, 118.0734],\n",
       "          [201.4834, 154.2543, 272.7257, 210.5002],\n",
       "          [191.8695, 195.2595, 199.5320, 204.4504],\n",
       "          [  0.0000, 161.6728,  15.7172, 177.0604]], grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([ 1,  1, 31, 75,  1, 31, 31, 31, 31,  1, 15,  1,  1,  1, 31, 44,  1, 27,\n",
       "          31, 31, 75, 15]),\n",
       "  'scores': tensor([0.9995, 0.9963, 0.9037, 0.8936, 0.6813, 0.5850, 0.4005, 0.3342, 0.3223,\n",
       "          0.2611, 0.1582, 0.1555, 0.1255, 0.1166, 0.1155, 0.0946, 0.0913, 0.0886,\n",
       "          0.0818, 0.0653, 0.0587, 0.0577], grad_fn=<IndexBackward0>)}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch,torchvision\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, progress=False)\n",
    "model.eval()\n",
    "prediction=model([x])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dfee581-fc76-43dc-9056-2585c09b7eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3facadc7-5d7d-44a8-9af8-d3d4db61ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_instance_segmentation(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927b049b-8bf6-4bbf-a42e-9a9cb2869a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0321, 0.0871, 0.2369, 0.6439],\n",
       "        [0.0321, 0.0871, 0.2369, 0.6439],\n",
       "        [0.0321, 0.0871, 0.2369, 0.6439]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "x=torch.Tensor([[1,2,3,4],[1,2,3,4],[1,2,3,4]])\n",
    "print(x)\n",
    "F.softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18464882-2baa-412d-9c76-faa5496882c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b70d9-5c79-4d3f-ade7-8b85db8347f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
