{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be77070c-f260-4765-b782-340a166ac1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aec849cf-caa3-4c06-be06-42fc6bf81ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1614.9081429692362\n",
      "199 1101.5135659195748\n",
      "299 753.0656880145582\n",
      "399 516.309736198957\n",
      "499 355.2649457482324\n",
      "599 245.5967702270655\n",
      "699 170.8302544745673\n",
      "799 119.7997819568718\n",
      "899 84.92992363748047\n",
      "999 61.075440693313766\n",
      "1099 44.73781266584258\n",
      "1199 33.535544451059536\n",
      "1299 25.84568226961672\n",
      "1399 20.560945985353396\n",
      "1499 16.925014223518026\n",
      "1599 14.420692092799854\n",
      "1699 12.693899590145714\n",
      "1799 11.501949396224997\n",
      "1899 10.678312720340706\n",
      "1999 10.108589816779588\n",
      "Result:y=0.032214758966576086+0.8381651865978047x+-0.005557581977637425x^2+-0.0906881636651305x^3\n",
      "Wall time: 322 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import math\n",
    "# Create random input and output data\n",
    "x=np.linspace(-math.pi,math.pi,2000)\n",
    "y=np.sin(x)\n",
    "# Randomly initialize weights\n",
    "a=np.random.randn()\n",
    "b=np.random.randn()\n",
    "c=np.random.randn()\n",
    "d=np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    #迭代2000次\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred=a+b*x+c*x**2+d*x**3\n",
    "    loss=np.square(y_pred-y).sum()\n",
    "    # compute and print loss\n",
    "    if t%100==99:\n",
    "        print(t,loss)\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred=2.0*(y_pred-y) #由损失函数求导转化而来，计算梯度必须要现有损失函数\n",
    "    grad_a=grad_y_pred.sum()\n",
    "    grad_b=(grad_y_pred*x).sum()\n",
    "    grad_c=(grad_y_pred*x**2).sum()\n",
    "    grad_d=(grad_y_pred*x**3).sum()\n",
    "    #update weights\n",
    "    a-=learning_rate * grad_a\n",
    "    b-=learning_rate * grad_b\n",
    "    c-=learning_rate * grad_c\n",
    "    d-=learning_rate * grad_d\n",
    "print(f'Result:y={a}+{b}x+{c}x^2+{d}x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ded3bc7-a98c-49a2-945e-f1a499c0cb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2303.4462890625\n",
      "199 1526.611328125\n",
      "299 1012.7960205078125\n",
      "399 672.93701171875\n",
      "499 448.1352844238281\n",
      "599 299.4358825683594\n",
      "699 201.07339477539062\n",
      "799 136.00643920898438\n",
      "899 92.96334838867188\n",
      "999 64.4886245727539\n",
      "1099 45.651084899902344\n",
      "1199 33.18854522705078\n",
      "1299 24.943374633789062\n",
      "1399 19.488147735595703\n",
      "1499 15.878701210021973\n",
      "1599 13.490352630615234\n",
      "1699 11.909989356994629\n",
      "1799 10.864179611206055\n",
      "1899 10.17208194732666\n",
      "1999 9.714048385620117\n",
      "Result: y = -0.0040124827064573765 + 0.8278629779815674 x + 0.0006922206957824528 x^2 + -0.08922275900840759 x^3\n",
      "Wall time: 691 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import math\n",
    "dtype=torch.float\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "x=torch.linspace(-math.pi,math.pi,2000,device=device,dtype=dtype)\n",
    "y=torch.sin(x)\n",
    "#randomly initialize wights\n",
    "a=torch.randn((),device=device,dtype=dtype)\n",
    "b=torch.randn((),device=device,dtype=dtype)\n",
    "c=torch.randn((),device=device,dtype=dtype)\n",
    "d=torch.randn((),device=device,dtype=dtype)\n",
    "learning_rate=1e-6\n",
    "for t in range(2000):\n",
    "    y_pred=a+b*x+c*x**2+d*x**3\n",
    "    #compute and print loss\n",
    "    loss=(y_pred-y).pow(2).sum().item()\n",
    "    if t%100==99:\n",
    "        print(t,loss)\n",
    "    \n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred=2.0*(y_pred-y)\n",
    "    grad_a=grad_y_pred.sum()\n",
    "    grad_b=(grad_y_pred*x).sum()\n",
    "    grad_c=(grad_y_pred*x**2).sum()\n",
    "    grad_d=(grad_y_pred*x**3).sum()\n",
    "    # update weights using gradient descent\n",
    "    a-=learning_rate * grad_a\n",
    "    b-=learning_rate * grad_b\n",
    "    c-=learning_rate * grad_c\n",
    "    d-=learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef6f48ba-fed2-4303-9bc6-1b595515be83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 448.6660461425781\n",
      "9999 65.90621948242188\n",
      "14999 16.77259063720703\n",
      "19999 9.877023696899414\n",
      "24999 8.979098320007324\n",
      "29999 8.59366226196289\n",
      "Result: y = -0.0021008430048823357 + 0.8537216186523438 x + -0.00015774245548527688 x^2 + -0.0932685062289238 x^3 + 0.00011178691784152761 x^4 ? + 0.00011178691784152761 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate five parameters and assign them as members.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 4, 5\n",
    "        and reuse the e parameter to compute the contribution of these orders.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
    "        times when defining a computational graph.\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 5000 == 4999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add6483-e1f8-41c2-988e-70f1713ebbea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
