{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ebd80cf-9ff2-4738-978b-3cbdeae64e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)  #nlayers 个encoder_layers\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)    #二维矩阵+偏置项\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:   #->None仅说明函数返回None，->用以说明函数返回类型\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)     # 执行PositionalEncoding的forward函数\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)  #triu生产三角矩阵，diagonal指定对角线位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f60a8232-340e-4d8f-90b7-5561e7207c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.triu(torch.ones(4,4) * float('-inf'),diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36f9476-35c3-4f9f-be7e-956f1d5407f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = transformer_encoder(src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d550e07-1feb-43b3-9b9c-9d31722a2920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 4, 5, 3],\n",
      "        [4, 3, 2, 9, 5]])\n",
      "torch.Size([2, 5, 3])\n",
      "torch.Size([2, 3, 5])\n",
      "tensor([[1, 2, 4, 5, 3],\n",
      "        [4, 3, 2, 9, 5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4, 3],\n",
       "        [4, 2, 5]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3)  #10对应参数input中的最大值\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1,2,4,5,3],[4,3,2,9,5]])\n",
    "output=embedding(input)\n",
    "print(input)\n",
    "print(output.shape)\n",
    "print(output.transpose(1,2).shape)\n",
    "print(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2043d715-2273-45cc-9098-45abe8828ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)  #随机按指定概率对矩阵元素设置为0\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)  #unsqueeze(1)一维数组转为2维\n",
    "        #torch.exp:e的次方，math.log:默认底数为e\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)  #register_buffer添加一个缓冲器，暂不执行\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec329622-b999-4b68-87a8-c2c2b8c1768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')  #tokenizer()将句子按照英文单词的进行拆分\n",
    "#迭代句子，构建单词和数字间的映射，后面需要用这种映射将英文句子转换成数字,specials指定特殊字符，特殊字符生成的数字最小\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])  \n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]    #把英语句子按vocab映射为数字，iterm为英文句子，\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))   #cat：连接相同的序列张量，不指定dim，则默认结果为1维\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "# batch_size = 20\n",
    "# eval_batch_size = 10\n",
    "# print(train_data.shape)\n",
    "# train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "# print(train_data.shape)\n",
    "# val_data = batchify(val_data, eval_batch_size)\n",
    "# test_data = batchify(test_data, eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d065793c-8929-4cf1-996a-c25db8a6ca64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   9,  632,    0,  ..., 7213,    0,    3])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in test_iter]\n",
    "torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6efe4390-210d-46fd-a9c4-423a4828b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = build_vocab_from_iterator(map(tokenizer, \"i am student\"), specials=['<unk>'])  \n",
    "tokenizer(\"i am student\")\n",
    "t=[torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in test_iter] \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4bb95361-3341-43a5-a6c7-fdd4696bf4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c977410-4384-4079-bca3-1e950a7834dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)       #reshape(-1)将多维矩阵转为一维矩阵\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e1a9ea-0ee9-4061-8d5a-0a01a4eb5822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 4., 2., 3., 5.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=torch.Tensor([[1,3,4],[2,3,5]])\n",
    "data.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb909f15-a43e-4182-b06f-8591b24e3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "663814ec-e3b4-46ce-a766-b4dc1eaaecef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28782"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00a764cb-2cf7-46e9-b5a8-3f5345141c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(28782, 200)\n",
       "  (decoder): Linear(in_features=200, out_features=28782, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c36363-f5de-4120-9c39-84fb2428b4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9622318f-8273-499b-b3c3-722b3c9d7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt    # // 求整数\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f68a339-101d-4c38-8996-7a6f2659ce9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 35\n",
      "2 70\n"
     ]
    }
   ],
   "source": [
    "for batch, i in enumerate(range(0, 100, bptt)):\n",
    "    print(batch,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "364de0d7-735d-409e-88e6-49bb013a9b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 346.76 | loss  8.11 | ppl  3318.03\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 342.45 | loss  6.88 | ppl   970.73\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 339.58 | loss  6.45 | ppl   632.49\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 320.95 | loss  6.31 | ppl   549.33\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 310.71 | loss  6.20 | ppl   490.38\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 309.14 | loss  6.16 | ppl   472.90\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 309.53 | loss  6.12 | ppl   453.66\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 307.71 | loss  6.11 | ppl   448.47\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 308.68 | loss  6.02 | ppl   411.33\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 310.71 | loss  6.02 | ppl   410.22\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 312.48 | loss  5.90 | ppl   364.65\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 311.39 | loss  5.97 | ppl   390.11\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 313.63 | loss  5.95 | ppl   384.71\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 15853.13 | loss  5.88 | ppl   356.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 4083.17s | valid loss  5.79 | valid ppl   327.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 305.64 | loss  5.86 | ppl   350.88\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 304.80 | loss  5.84 | ppl   344.09\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 304.84 | loss  5.66 | ppl   286.60\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 308.78 | loss  5.70 | ppl   298.80\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 311.26 | loss  5.65 | ppl   285.71\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 311.39 | loss  5.68 | ppl   291.90\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 310.49 | loss  5.69 | ppl   295.40\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 98269.73 | loss  5.71 | ppl   302.56\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 378.81 | loss  5.65 | ppl   285.14\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 367.37 | loss  5.67 | ppl   291.23\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 371.27 | loss  5.55 | ppl   257.79\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 409.16 | loss  5.65 | ppl   284.14\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 423.81 | loss  5.64 | ppl   282.76\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 419.51 | loss  5.58 | ppl   266.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 20674.15s | valid loss  5.67 | valid ppl   290.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 416.67 | loss  5.60 | ppl   271.58\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 431.76 | loss  5.61 | ppl   273.94\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 425.81 | loss  5.43 | ppl   227.64\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 427.43 | loss  5.48 | ppl   240.23\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 426.36 | loss  5.43 | ppl   228.05\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 422.44 | loss  5.47 | ppl   237.12\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 420.92 | loss  5.48 | ppl   240.48\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 418.39 | loss  5.52 | ppl   249.19\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 425.54 | loss  5.46 | ppl   235.29\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 444.86 | loss  5.47 | ppl   237.49\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 433.07 | loss  5.35 | ppl   209.72\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 449.86 | loss  5.45 | ppl   232.93\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 450.44 | loss  5.46 | ppl   234.26\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 591.78 | loss  5.39 | ppl   220.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1378.75s | valid loss  5.61 | valid ppl   273.52\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
